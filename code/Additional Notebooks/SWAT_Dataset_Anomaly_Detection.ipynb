{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2ZlgfXbfM-N"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers, losses\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"SWaT_Normal.csv\")\n",
        "normal = df.drop([\" Timestamp\" , \"Normal/Attack\" ] , axis = 1)\n",
        "normal.shape\n",
        "\n",
        "df_attack  = pd.read_csv(\"SWaT_Attack.csv\")\n",
        "y_test1 = df_attack[\"Normal/Attack\"]\n",
        "attack = df_attack.drop([\" Timestamp\" , \"Normal/Attack\" ] , axis = 1)\n",
        "attack.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7jFDn3JfntG",
        "outputId": "80f322e2-4754-402f-fabc-e43243d97f74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(449919, 51)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "normal = normal.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)"
      ],
      "metadata": {
        "id": "iq_7cYhEfxJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing\n",
        "'''\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "x = normal.values\n",
        "x_scaled = min_max_scaler.fit_transform(x)\n",
        "normal = pd.DataFrame(x_scaled)\n",
        "from sklearn import preprocessing\n",
        "\n",
        "x = attack.values \n",
        "x_scaled = min_max_scaler.transform(x)\n",
        "attack = pd.DataFrame(x_scaled)\n",
        "'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "uWg7zRY9fyIr",
        "outputId": "c726b464-d41d-4030-a95a-9c7e1d664588"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nmin_max_scaler = preprocessing.MinMaxScaler()\\nx = normal.values\\nx_scaled = min_max_scaler.fit_transform(x)\\nnormal = pd.DataFrame(x_scaled)\\nfrom sklearn import preprocessing\\n\\nx = attack.values \\nx_scaled = min_max_scaler.transform(x)\\nattack = pd.DataFrame(x_scaled)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#generating normalized train/test datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "def fit_scaler(data, scaler_type=MinMaxScaler):\n",
        "  scaler = scaler_type()\n",
        "  scaler.fit(data)\n",
        "  return scaler\n",
        "\n",
        "def generate_datasets_for_training(data, window_size, scaler):\n",
        "  _l = len(data) \n",
        "  #normalizing values\n",
        "  data = scaler.transform(data)\n",
        "  Xs = []\n",
        "  Ys = []\n",
        "  for i in range(0, (_l - window_size)):\n",
        "    # because this is an autoencoder - our Ys are the same as our Xs. No need to pull the next sequence of values\n",
        "    Xs.append(data[i:i+window_size])\n",
        "    Ys.append(data[i:i+window_size])\n",
        "  X_train, X_test, Y_train, Y_test = [np.array(x) for x in train_test_split(Xs, Ys, train_size = 0.7, shuffle=False)]\n",
        "  assert X_train.shape[2] == X_test.shape[2] == (data.shape[1] if (type(data) == np.ndarray) else len(data))\n",
        "  return  (X_train.shape[2], X_train, X_test, Y_train, Y_test)"
      ],
      "metadata": {
        "id": "1V7PRpW2f4_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prep\n",
        "epochs = 50\n",
        "batch_size = 128 #originally 32\n",
        "window_size = 60\n",
        "from keras import metrics\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "#training can stop early at some minimum error threshold to avoid overfitting\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss', min_delta=1e-2, patience=5, verbose=0, mode='auto',\n",
        "    baseline=None, restore_best_weights=True)\n",
        "\n",
        "model_scaler = fit_scaler(normal)\n",
        "feats, X, XX, Y, YY = generate_datasets_for_training(data=normal, window_size=window_size, scaler=model_scaler)"
      ],
      "metadata": {
        "id": "ZnlBuwjZhMSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6U-U3Xj1lRIi",
        "outputId": "39eb0a62-08de-4021-e780-fa3aa91cbc96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.00000000e+00, 5.29434115e-03, 5.00000000e-01, 0.00000000e+00,\n",
              "       0.00000000e+00, 1.22914080e-02, 7.50992639e-02, 2.00908339e-03,\n",
              "       0.00000000e+00, 5.00000000e-01, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       1.21377629e-01, 1.08624650e-04, 6.44921706e-03, 5.00000000e-01,\n",
              "       5.00000000e-01, 5.00000000e-01, 5.00000000e-01, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 1.88673672e-01, 0.00000000e+00,\n",
              "       3.96167160e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 6.79975314e-02, 4.35223511e-01,\n",
              "       2.75615527e-01, 5.27549303e-01, 1.45942525e-04, 5.64546374e-04,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       8.14383482e-04, 0.00000000e+00, 1.21664406e-03, 1.46783947e-04,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#defining and training the model\n",
        "\n",
        "model = keras.Sequential()\n",
        "\n",
        "#encoder layers\n",
        "model.add(keras.layers.LSTM(64, kernel_initializer='he_uniform', batch_input_shape=(None, window_size, feats), return_sequences=True, name='encoder_1'))\n",
        "model.add(keras.layers.LSTM(32, kernel_initializer='he_uniform', return_sequences=True, name='encoder_2'))\n",
        "model.add(keras.layers.LSTM(16, kernel_initializer='he_uniform', return_sequences=False, name='encoder_3'))\n",
        "\n",
        "#repeat vector: formats encoded vector so it's readable to the decoder\n",
        "model.add(keras.layers.RepeatVector(window_size, name='encoder_decoder_bridge'))\n",
        "\n",
        "#decoder\n",
        "model.add(keras.layers.LSTM(16, kernel_initializer='he_uniform', return_sequences=True, name='decoder_1'))\n",
        "model.add(keras.layers.LSTM(32, kernel_initializer='he_uniform', return_sequences=True, name='decoder_2'))\n",
        "model.add(keras.layers.LSTM(64, kernel_initializer='he_uniform', return_sequences=True, name='decoder_3'))\n",
        "\n",
        "#dense layer produces sequence similar to input\n",
        "model.add(keras.layers.TimeDistributed(keras.layers.Dense(feats)))\n",
        "\n",
        "model.compile(loss=\"mse\",optimizer='adam')\n",
        "model.build()\n",
        "print(model.summary())\n",
        "\n",
        "#model.fit(x=X_train, y=Y_train, validation_data=(X_test, Y_test), epochs=epochs, batch_size=batch_size, shuffle=True, callbacks=[early_stop])\n",
        "history = model.fit(x=X, y=Y, validation_data=(XX, YY), epochs=2, batch_size=batch_size, shuffle=True, callbacks=[early_stop])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7twqeG8EhQFH",
        "outputId": "8b751466-9279-40f5-cec4-aa7721d96138"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " encoder_1 (LSTM)            (None, 60, 64)            29696     \n",
            "                                                                 \n",
            " encoder_2 (LSTM)            (None, 60, 32)            12416     \n",
            "                                                                 \n",
            " encoder_3 (LSTM)            (None, 16)                3136      \n",
            "                                                                 \n",
            " encoder_decoder_bridge (Rep  (None, 60, 16)           0         \n",
            " eatVector)                                                      \n",
            "                                                                 \n",
            " decoder_1 (LSTM)            (None, 60, 16)            2112      \n",
            "                                                                 \n",
            " decoder_2 (LSTM)            (None, 60, 32)            6272      \n",
            "                                                                 \n",
            " decoder_3 (LSTM)            (None, 60, 64)            24832     \n",
            "                                                                 \n",
            " time_distributed (TimeDistr  (None, 60, 51)           3315      \n",
            " ibuted)                                                         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 81,779\n",
            "Trainable params: 81,779\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/2\n",
            "46/46 [==============================] - 35s 461ms/step - loss: 0.1457 - val_loss: 0.0466\n",
            "Epoch 2/2\n",
            "46/46 [==============================] - 20s 436ms/step - loss: 0.0432 - val_loss: 0.0288\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#generating normalized train/test datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "def fit_scaler(data, scaler_type=MinMaxScaler):\n",
        "  scaler = scaler_type()\n",
        "  scaler.fit(data)\n",
        "  return scaler\n",
        "\n",
        "def generate_datasets_for_training(data, window_size, scaler):\n",
        "  _l = len(data) \n",
        "  #normalizing values\n",
        "  data = scaler.transform(data)\n",
        "  Xs = []\n",
        "  Ys = []\n",
        "  i = 0\n",
        "  while i <=  (_l - window_size):\n",
        "    # because this is an autoencoder - our Ys are the same as our Xs. No need to pull the next sequence of values\n",
        "    Xs.append(data[i:i+window_size])\n",
        "    Ys.append(data[i:i+window_size])\n",
        "    i += window_size\n",
        "  X_train, X_test, Y_train, Y_test = [np.array(x) for x in train_test_split(Xs, Ys, train_size = 0.7, shuffle=False)]\n",
        "  assert X_train.shape[2] == X_test.shape[2] == (data.shape[1] if (type(data) == np.ndarray) else len(data))\n",
        "  return  (X_train.shape[2], X_train, X_test, Y_train, Y_test)\n"
      ],
      "metadata": {
        "id": "nhuu82pMiVLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_dataset_for_testing(data, window_size, scaler):\n",
        "  _l = len(data)\n",
        "  #normalizing values\n",
        "  data = scaler.transform(data)\n",
        "  Xs = []\n",
        "  i = 0\n",
        "  while i <=  (_l - window_size):\n",
        "    # because this is an autoencoder - our Ys are the same as our Xs. No need to pull the next sequence of values\n",
        "    Xs.append(data[i:i+window_size])\n",
        "    i += window_size\n",
        "  return np.array(Xs)\n"
      ],
      "metadata": {
        "id": "3Y8kuWcWji8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#format test data using scaler from training\n",
        "X_test = format_dataset_for_testing(data=attack, window_size=window_size, scaler=model_scaler)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ReK_oAX4mPNr",
        "outputId": "8a891221-653b-44b9-9e75-d53c00fc10d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:493: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
            "Feature names unseen at fit time:\n",
            "-  AIT201\n",
            "-  MV101\n",
            "-  MV201\n",
            "-  MV303\n",
            "-  P201\n",
            "- ...\n",
            "Feature names seen at fit time, yet now missing:\n",
            "- AIT201\n",
            "- MV101\n",
            "- MV201\n",
            "- MV303\n",
            "- P201\n",
            "- ...\n",
            "\n",
            "  warnings.warn(message, FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_X_raw = model.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXdQO-RzmSm0",
        "outputId": "7e059c0e-7ec9-4a05-c2bf-fc1d5748d920"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "235/235 [==============================] - 13s 53ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score = model.evaluate(pred_X_raw, X_test, batch_size=batch_size, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIiYvQCTnL6g",
        "outputId": "c2c2b820-327c-4a47-e938-92c25504ecdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "59/59 [==============================] - 8s 127ms/step - loss: 0.3139\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_dnn_sigmoid  = dnn.predict(x_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "lek27-LTovT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_X_raw[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6PL-mGhpjam",
        "outputId": "d1f4e20b-3ba8-4252-b707-97840d390bb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.10392685,  0.11519286,  0.12188097,  0.08695778,  0.0088117 ,\n",
              "        0.10568662,  0.03748055,  0.01497934,  0.10604897,  0.16813393,\n",
              "       -0.0027255 ,  0.01606499,  0.08006833,  0.03575063,  0.15675598,\n",
              "        0.0172257 ,  0.03483454,  0.14730209,  0.15896496,  0.13002871,\n",
              "        0.16713558,  0.09007151,  0.10421488,  0.00551953,  0.14206356,\n",
              "        0.13431819,  0.07423939,  0.13465554,  0.1497671 , -0.01918647,\n",
              "        0.14794129,  0.00980964, -0.0059814 ,  0.13451864,  0.14368401,\n",
              "        0.06272632,  0.10939229,  0.01518664,  0.15880197,  0.15224233,\n",
              "        0.17693593,  0.18606102,  0.1947558 ,  0.00115978,  0.17024875,\n",
              "        0.12163817,  0.16605915,  0.02333777, -0.03909359,  0.03619824,\n",
              "       -0.01641725], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "windows_normal=normal.values[np.arange(window_size)[None, :] + np.arange(normal.shape[0]-window_size)[:, None]]\n",
        "windows_normal.shape"
      ],
      "metadata": {
        "id": "ZmqfZP85pkka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import abc\n",
        "import logging\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class Algorithm(metaclass=abc.ABCMeta):\n",
        "    def __init__(self, module_name, name, seed, details=False):\n",
        "        self.logger = logging.getLogger(module_name)\n",
        "        self.name = name\n",
        "        self.seed = seed\n",
        "        self.details = details\n",
        "        self.prediction_details = {}\n",
        "\n",
        "        if self.seed is not None:\n",
        "            random.seed(seed)\n",
        "            np.random.seed(seed)\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.name\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def fit(self, X):\n",
        "        \"\"\"\n",
        "        Train the algorithm on the given dataset\n",
        "        \"\"\"\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        :return anomaly score\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "class PyTorchUtils(metaclass=abc.ABCMeta):\n",
        "    def __init__(self, seed, gpu):\n",
        "        self.gpu = gpu\n",
        "        self.seed = seed\n",
        "        if self.seed is not None:\n",
        "            torch.manual_seed(self.seed)\n",
        "            torch.cuda.manual_seed(self.seed)\n",
        "        self.framework = 0\n",
        "\n",
        "    @property\n",
        "    def device(self):\n",
        "        return torch.device(f'cuda:{self.gpu}' if torch.cuda.is_available() and self.gpu is not None else 'cpu')\n",
        "\n",
        "    def to_var(self, t, **kwargs):\n",
        "        # ToDo: check whether cuda Variable.\n",
        "        t = t.to(self.device)\n",
        "        return Variable(t, **kwargs)\n",
        "\n",
        "    def to_device(self, model):\n",
        "        model.to(self.device)"
      ],
      "metadata": {
        "id": "V9-5PvDObA05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch-geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 684
        },
        "id": "HY1o0K4ebL1p",
        "outputId": "039ed90a-953d-43c7-c61d-bb6a655e48dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.2.0.tar.gz (564 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m565.0/565.0 KB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (4.64.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (1.7.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (2.25.1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (3.0.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (1.0.2)\n",
            "Collecting psutil>=5.8.0\n",
            "  Downloading psutil-5.9.4-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (280 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.2/280.2 KB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch-geometric) (2.0.1)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torch-geometric) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torch-geometric) (2022.12.7)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->torch-geometric) (1.2.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.2.0-py3-none-any.whl size=773302 sha256=0f6d6ae37b86a6c77f9793dcdb4d0bfea0a96f7d85fd4b98f40b1724939b3b5f\n",
            "  Stored in directory: /root/.cache/pip/wheels/59/a3/20/198928106d3169865ae73afcbd3d3d1796cf6b429b55c65378\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: psutil, torch-geometric\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.4.8\n",
            "    Uninstalling psutil-5.4.8:\n",
            "      Successfully uninstalled psutil-5.4.8\n",
            "Successfully installed psutil-5.9.4 torch-geometric-2.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "psutil"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch-sparse"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AI--hZO9bXCV",
        "outputId": "44cb2b2d-d96c-4cde-866d-7c9087301edd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch-sparse\n",
            "  Downloading torch_sparse-0.6.16.tar.gz (208 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.2/208.2 KB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from torch-sparse) (1.7.3)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.8/dist-packages (from scipy->torch-sparse) (1.21.6)\n",
            "Building wheels for collected packages: torch-sparse\n",
            "  Building wheel for torch-sparse (setup.py) ... \u001b[?25l\u001b[?25hcanceled\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mTraceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/base_command.py\", line 167, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/req_command.py\", line 199, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/commands/install.py\", line 361, in run\n",
            "    _, build_failures = build(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/wheel_builder.py\", line 348, in build\n",
            "    wheel_file = _build_one(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/wheel_builder.py\", line 222, in _build_one\n",
            "    wheel_path = _build_one_inside_env(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/wheel_builder.py\", line 269, in _build_one_inside_env\n",
            "    wheel_path = build_wheel_legacy(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/operations/build/wheel_legacy.py\", line 83, in build_wheel_legacy\n",
            "    output = call_subprocess(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/utils/subprocess.py\", line 166, in call_subprocess\n",
            "    line: str = proc.stdout.readline()\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/main.py\", line 70, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/base_command.py\", line 221, in _main\n",
            "    return run(options, args)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/base_command.py\", line 205, in exc_logging_wrapper\n",
            "    logger.debug(\"Exception information:\", exc_info=True)\n",
            "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1434, in debug\n",
            "    self._log(DEBUG, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1589, in _log\n",
            "    self.handle(record)\n",
            "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1599, in handle\n",
            "    self.callHandlers(record)\n",
            "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1661, in callHandlers\n",
            "    hdlr.handle(record)\n",
            "  File \"/usr/lib/python3.8/logging/__init__.py\", line 954, in handle\n",
            "    self.emit(record)\n",
            "  File \"/usr/lib/python3.8/logging/handlers.py\", line 71, in emit\n",
            "    logging.FileHandler.emit(self, record)\n",
            "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1187, in emit\n",
            "    StreamHandler.emit(self, record)\n",
            "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1085, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.8/logging/__init__.py\", line 929, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/utils/logging.py\", line 110, in format\n",
            "    formatted = super().format(record)\n",
            "  File \"/usr/lib/python3.8/logging/__init__.py\", line 676, in format\n",
            "    record.exc_text = self.formatException(record.exc_info)\n",
            "  File \"/usr/lib/python3.8/logging/__init__.py\", line 626, in formatException\n",
            "    traceback.print_exception(ei[0], ei[1], tb, None, sio)\n",
            "  File \"/usr/lib/python3.8/traceback.py\", line 103, in print_exception\n",
            "    for line in TracebackException(\n",
            "  File \"/usr/lib/python3.8/traceback.py\", line 508, in __init__\n",
            "    self.stack = StackSummary.extract(\n",
            "  File \"/usr/lib/python3.8/traceback.py\", line 366, in extract\n",
            "    f.line\n",
            "  File \"/usr/lib/python3.8/traceback.py\", line 288, in line\n",
            "    self._line = linecache.getline(self.filename, self.lineno).strip()\n",
            "  File \"/usr/lib/python3.8/linecache.py\", line 16, in getline\n",
            "    lines = getlines(filename, module_globals)\n",
            "  File \"/usr/lib/python3.8/linecache.py\", line 47, in getlines\n",
            "    return updatecache(filename, module_globals)\n",
            "  File \"/usr/lib/python3.8/linecache.py\", line 136, in updatecache\n",
            "    with tokenize.open(fullname) as fp:\n",
            "  File \"/usr/lib/python3.8/tokenize.py\", line 394, in open\n",
            "    encoding, lines = detect_encoding(buffer.readline)\n",
            "  File \"/usr/lib/python3.8/tokenize.py\", line 363, in detect_encoding\n",
            "    first = read_or_stop()\n",
            "  File \"/usr/lib/python3.8/tokenize.py\", line 321, in read_or_stop\n",
            "    return readline()\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch\n",
        "from torch_geometric.nn import GCNConv, GATConv, GraphConv\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data, Batch\n",
        "\n",
        "\n",
        "\n",
        "class GCNLSTMCell(nn.Module, PyTorchUtils):\n",
        "\n",
        "    def __init__(self, nodes_num, input_dim, hidden_dim, bias=True, seed: int=0, gpu: int=None):\n",
        "        \"\"\"\n",
        "        Initialize GCNLSTM cell.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        nodes_num: input\n",
        "            Number of nodes.\n",
        "        input_dim: int\n",
        "            Number of channels of input tensor.\n",
        "        hidden_dim: int\n",
        "            Number of channels of hidden state.\n",
        "        bias: bool\n",
        "            Whether or not to add the bias.\n",
        "        \"\"\"\n",
        "\n",
        "        super(GCNLSTMCell, self).__init__()\n",
        "        PyTorchUtils.__init__(self, seed, gpu)\n",
        "\n",
        "        self.nodes_num = nodes_num\n",
        "        self.input_dim  = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.bias = bias\n",
        "        \n",
        "        self.gconv = GCNConv(in_channels=self.input_dim + self.hidden_dim,\n",
        "                             out_channels=4 * self.hidden_dim,\n",
        "                             bias=self.bias,\n",
        "                             improved = True)\n",
        "\n",
        "    def forward(self, input_tensor, cur_state, edge_index):\n",
        "        '''\n",
        "        input_tensor:(b,n,i)\n",
        "        cur_state:[(b,n,h),(b,n,h)]\n",
        "        '''\n",
        "        h_cur, c_cur = cur_state\n",
        "        \n",
        "        combined = torch.cat([input_tensor, h_cur], dim=2)  # concatenate along hidden axis\n",
        "        batch = Batch.from_data_list([Data(x=combined[i], edge_index=edge_index) for i in range(combined.shape[0])])\n",
        "        \n",
        "        combined_conv = self.gconv(batch.x, batch.edge_index)\n",
        "        combined_conv = combined_conv.reshape(combined.shape[0],combined.shape[1],-1)\n",
        "\n",
        "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=2) \n",
        "        i = torch.sigmoid(cc_i)\n",
        "        f = torch.sigmoid(cc_f)\n",
        "        o = torch.sigmoid(cc_o)\n",
        "        g = torch.tanh(cc_g)\n",
        "\n",
        "        c_next = f * c_cur + i * g\n",
        "        h_next = o * torch.tanh(c_next)\n",
        "\n",
        "        # output: (b,n,h),(b,n,h)\n",
        "        return h_next, c_next\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return (self.to_var(Variable(torch.zeros(batch_size, self.nodes_num, self.hidden_dim))),\n",
        "                self.to_var(Variable(torch.zeros(batch_size, self.nodes_num, self.hidden_dim))))\n",
        "\n",
        "class GATLSTMCell(nn.Module, PyTorchUtils):\n",
        "\n",
        "    def __init__(self, nodes_num, input_dim, hidden_dim, head=1, dropout=0, bias=True, seed: int=0, gpu: int=None):\n",
        "        \"\"\"\n",
        "        Initialize GCNLSTM cell.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        nodes_num: input\n",
        "            Number of nodes.\n",
        "        input_dim: int\n",
        "            Number of channels of input tensor.\n",
        "        hidden_dim: int\n",
        "            Number of channels of hidden state.\n",
        "        head: int\n",
        "            Number of multi-head-attentions.\n",
        "        dropout: float\n",
        "            Dropout probability of the normalized attention coefficients.\n",
        "        bias: bool\n",
        "            Whether or not to add the bias.\n",
        "        \"\"\"\n",
        "\n",
        "        super(GATLSTMCell, self).__init__()\n",
        "        PyTorchUtils.__init__(self, seed, gpu)\n",
        "\n",
        "        self.nodes_num = nodes_num\n",
        "        self.input_dim  = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.head = head\n",
        "        self.dropout = dropout\n",
        "        self.bias = bias\n",
        "        \n",
        "        self.gconv = GATConv(in_channels=self.input_dim + self.hidden_dim,\n",
        "                             out_channels=4 * self.hidden_dim,\n",
        "                             heads=self.head,\n",
        "                             concat = False,\n",
        "                             dropout=self.dropout,\n",
        "                             bias=self.bias)\n",
        "\n",
        "    def forward(self, input_tensor, cur_state, edge_index):\n",
        "        '''\n",
        "        input_tensor:(b,n,i)\n",
        "        cur_state:[(b,n,h),(b,n,h)]\n",
        "        '''\n",
        "        h_cur, c_cur = cur_state\n",
        "\n",
        "        combined = torch.cat([input_tensor, h_cur], dim=2)  # concatenate along hidden axis\n",
        "        batch = Batch.from_data_list([Data(x=combined[i], edge_index=edge_index) for i in range(combined.shape[0])])\n",
        "        \n",
        "        combined_conv = self.gconv(batch.x, batch.edge_index)\n",
        "        combined_conv = combined_conv.reshape(combined.shape[0],combined.shape[1],-1)\n",
        "\n",
        "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=2) \n",
        "        i = torch.sigmoid(cc_i)\n",
        "        f = torch.sigmoid(cc_f)\n",
        "        o = torch.sigmoid(cc_o)\n",
        "        g = torch.tanh(cc_g)\n",
        "\n",
        "        c_next = f * c_cur + i * g\n",
        "        h_next = o * torch.tanh(c_next)\n",
        "        \n",
        "        # output: (b,n,h),(b,n,h)\n",
        "        return h_next, c_next\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return (self.to_var(Variable(torch.zeros(batch_size, self.nodes_num, self.hidden_dim))),\n",
        "                self.to_var(Variable(torch.zeros(batch_size, self.nodes_num, self.hidden_dim))))\n",
        "\n",
        "class WL1LSTMCell(nn.Module, PyTorchUtils):\n",
        "\n",
        "    def __init__(self, nodes_num, input_dim, hidden_dim, bias=True, seed: int=0, gpu: int=None):\n",
        "        \"\"\"\n",
        "        Initialize GCNLSTM cell.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        nodes_num: input\n",
        "            Number of nodes.\n",
        "        input_dim: int\n",
        "            Number of channels of input tensor.\n",
        "        hidden_dim: int\n",
        "            Number of channels of hidden state.\n",
        "        bias: bool\n",
        "            Whether or not to add the bias.\n",
        "        \"\"\"\n",
        "\n",
        "        super(WL1LSTMCell, self).__init__()\n",
        "        PyTorchUtils.__init__(self, seed, gpu)\n",
        "\n",
        "        self.nodes_num = nodes_num\n",
        "        self.input_dim  = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.bias = bias\n",
        "        \n",
        "        self.gconv = GraphConv(in_channels=self.input_dim + self.hidden_dim,\n",
        "                               out_channels=4 * self.hidden_dim,\n",
        "                               aggr = 'mean',\n",
        "                               bias=self.bias)\n",
        "\n",
        "    def forward(self, input_tensor, cur_state, edge_index):\n",
        "        '''\n",
        "        input_tensor:(b,n,i)\n",
        "        cur_state:[(b,n,h),(b,n,h)]\n",
        "        '''\n",
        "        h_cur, c_cur = cur_state\n",
        "        \n",
        "        combined = torch.cat([input_tensor, h_cur], dim=2)  # concatenate along hidden axis\n",
        "        batch = Batch.from_data_list([Data(x=combined[i], edge_index=edge_index) for i in range(combined.shape[0])])\n",
        "        \n",
        "        combined_conv = self.gconv(batch.x, batch.edge_index)\n",
        "        combined_conv = combined_conv.reshape(combined.shape[0],combined.shape[1],-1)\n",
        "\n",
        "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=2) \n",
        "        i = torch.sigmoid(cc_i)\n",
        "        f = torch.sigmoid(cc_f)\n",
        "        o = torch.sigmoid(cc_o)\n",
        "        g = torch.tanh(cc_g)\n",
        "\n",
        "        c_next = f * c_cur + i * g\n",
        "        h_next = o * torch.tanh(c_next)\n",
        "\n",
        "        # output: (b,n,h),(b,n,h)\n",
        "        return h_next, c_next\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return (self.to_var(Variable(torch.zeros(batch_size, self.nodes_num, self.hidden_dim))),\n",
        "                self.to_var(Variable(torch.zeros(batch_size, self.nodes_num, self.hidden_dim))))\n",
        "\n",
        "class LSTMCell(nn.Module, PyTorchUtils):\n",
        "\n",
        "    def __init__(self, nodes_num, input_dim, hidden_dim, bias=True, seed: int=0, gpu: int=None):\n",
        "        \"\"\"\n",
        "        Initialize GCNLSTM cell.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        nodes_num: input\n",
        "            Number of nodes.\n",
        "        input_dim: int\n",
        "            Number of channels of input tensor.\n",
        "        hidden_dim: int\n",
        "            Number of channels of hidden state.\n",
        "        bias: bool\n",
        "            Whether or not to add the bias.\n",
        "        \"\"\"\n",
        "\n",
        "        super(LSTMCell, self).__init__()\n",
        "        PyTorchUtils.__init__(self, seed, gpu)\n",
        "\n",
        "        self.nodes_num = nodes_num\n",
        "        self.input_dim  = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.bias = bias\n",
        "        \n",
        "        self.gconv = nn.Linear(self.input_dim + self.hidden_dim,\n",
        "                               4 * self.hidden_dim,\n",
        "                               bias=self.bias)\n",
        "\n",
        "    def forward(self, input_tensor, cur_state, edge_index):\n",
        "        '''\n",
        "        input_tensor:(b,n,i)\n",
        "        cur_state:[(b,n,h),(b,n,h)]\n",
        "        '''\n",
        "        h_cur, c_cur = cur_state\n",
        "        \n",
        "        combined = torch.cat([input_tensor, h_cur], dim=2)  # concatenate along hidden axis\n",
        "        #batch = Batch.from_data_list([Data(x=combined[i], edge_index=edge_index) for i in range(combined.shape[0])])\n",
        "        \n",
        "        combined_conv = self.gconv(combined)\n",
        "        #combined_conv = combined_conv.reshape(combined.shape[0],combined.shape[1],-1)\n",
        "\n",
        "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=2) \n",
        "        i = torch.sigmoid(cc_i)\n",
        "        f = torch.sigmoid(cc_f)\n",
        "        o = torch.sigmoid(cc_o)\n",
        "        g = torch.tanh(cc_g)\n",
        "\n",
        "        c_next = f * c_cur + i * g\n",
        "        h_next = o * torch.tanh(c_next)\n",
        "\n",
        "        # output: (b,n,h),(b,n,h)\n",
        "        return h_next, c_next\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return (self.to_var(Variable(torch.zeros(batch_size, self.nodes_num, self.hidden_dim))),\n",
        "                self.to_var(Variable(torch.zeros(batch_size, self.nodes_num, self.hidden_dim))))\n",
        "\n",
        "class GraphLSTM(nn.Module, PyTorchUtils):\n",
        "\n",
        "    def __init__(self, nodes_num, input_dim, hidden_dim, num_layers, head=1, dropout=0, kind='GCN',\n",
        "                 batch_first=False, bias=True, return_all_layers=True, seed: int=0, gpu: int=None):\n",
        "        super(GraphLSTM, self).__init__()\n",
        "        PyTorchUtils.__init__(self, seed, gpu)\n",
        "\n",
        "        # Make sure that `hidden_dim` are lists having len == num_layers\n",
        "        hidden_dim  = self._extend_for_multilayer(hidden_dim, num_layers)\n",
        "        head = self._extend_for_multilayer(head, num_layers)\n",
        "        if not len(hidden_dim) == len(head) == num_layers:\n",
        "            raise ValueError('Inconsistent list length.')\n",
        "\n",
        "        self.nodes_num = nodes_num\n",
        "        self.input_dim  = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.head = head\n",
        "        self.dropout = dropout\n",
        "        self.kind = kind\n",
        "        self.batch_first = batch_first\n",
        "        self.bias = bias\n",
        "        self.return_all_layers = return_all_layers\n",
        "\n",
        "        cell_list = []\n",
        "        for i in range(0, self.num_layers):\n",
        "            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i-1]\n",
        "\n",
        "            if self.kind == 'GCN':\n",
        "                cell_list.append(GCNLSTMCell(nodes_num=nodes_num,\n",
        "                                             input_dim=cur_input_dim,\n",
        "                                             hidden_dim=self.hidden_dim[i],\n",
        "                                             bias=self.bias,\n",
        "                                             seed=self.seed,\n",
        "                                             gpu=self.gpu))\n",
        "            elif self.kind == 'GAT':\n",
        "                cell_list.append(GATLSTMCell(nodes_num=nodes_num,\n",
        "                                             input_dim=cur_input_dim,\n",
        "                                             hidden_dim=self.hidden_dim[i],\n",
        "                                             head=self.head[i],\n",
        "                                             dropout=self.dropout,\n",
        "                                             bias=self.bias,\n",
        "                                             seed=self.seed,\n",
        "                                             gpu=self.gpu))\n",
        "            elif self.kind == 'WL1':\n",
        "                cell_list.append(WL1LSTMCell(nodes_num=nodes_num,\n",
        "                                             input_dim=cur_input_dim,\n",
        "                                             hidden_dim=self.hidden_dim[i],\n",
        "                                             bias=self.bias,\n",
        "                                             seed=self.seed,\n",
        "                                             gpu=self.gpu))\n",
        "            elif self.kind == 'LIN':\n",
        "                cell_list.append(LSTMCell(nodes_num=nodes_num,\n",
        "                                          input_dim=cur_input_dim,\n",
        "                                          hidden_dim=self.hidden_dim[i],\n",
        "                                          bias=self.bias,\n",
        "                                          seed=self.seed,\n",
        "                                          gpu=self.gpu))\n",
        "            else:\n",
        "                raise NotImplementedError()\n",
        "\n",
        "        self.cell_list = nn.ModuleList(cell_list)\n",
        "\n",
        "    def forward(self, input_tensor, edge_index, hidden_state=None):\n",
        "        \"\"\"\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        input_tensor: 4-D Tensor either of shape (t, b, n, h) or (b, t, n, h)\n",
        "        hidden_state: list [[(b, n, h), (b, n, h)]] * num_layers\n",
        "            \n",
        "        Returns\n",
        "        -------\n",
        "        last_output_list: [(b, t, n, h)] * num_layers(also determined by return_all_layers),\n",
        "        layer_state_list: [[(b, n, h), (b, n, h)]] * num_layers(also determined by return_all_layers)\n",
        "        \"\"\"\n",
        "        #确保(t, b, n, c)\n",
        "        #if self.batch_first:\n",
        "        #写在前面了\n",
        "            # (b, t, n, c) -> (t, b, n, c)\n",
        "            #input_tensor = input_tensor.permute(1, 0, 2, 3).contiguous()\n",
        "\n",
        "        # Implement stateful GraphLSTM\n",
        "        if hidden_state is not None:\n",
        "            hidden_state = hidden_state\n",
        "        else:\n",
        "            # [[(b, n, h), (b, n, h)]] * num_layers\n",
        "            hidden_state = self._init_hidden(input_tensor.size(1))\n",
        "\n",
        "        layer_output_list = []\n",
        "        last_state_list   = []\n",
        "\n",
        "        seq_len = input_tensor.size(0)\n",
        "        cur_layer_input = input_tensor\n",
        "\n",
        "        for layer_idx in range(self.num_layers):\n",
        "\n",
        "            h, c = hidden_state[layer_idx]\n",
        "            output_inner = []\n",
        "            for t in range(seq_len):\n",
        "\n",
        "                h, c = self.cell_list[layer_idx](input_tensor=cur_layer_input[t],\n",
        "                                                 edge_index = edge_index, cur_state=[h, c])\n",
        "                output_inner.append(h)\n",
        "\n",
        "            layer_output = torch.stack(output_inner, dim=0)\n",
        "            cur_layer_input = layer_output\n",
        "\n",
        "            layer_output_list.append(layer_output)\n",
        "            last_state_list.append([h, c])\n",
        "\n",
        "        if not self.return_all_layers:\n",
        "            layer_output_list = layer_output_list[-1:]\n",
        "            last_state_list   = last_state_list[-1:]\n",
        "\n",
        "        return layer_output_list, last_state_list\n",
        "\n",
        "    def _init_hidden(self, batch_size):\n",
        "        init_states = []\n",
        "        for i in range(self.num_layers):\n",
        "            init_states.append(self.cell_list[i].init_hidden(batch_size))\n",
        "        return init_states\n",
        "\n",
        "    @staticmethod\n",
        "    def _extend_for_multilayer(param, num_layers):\n",
        "        if not isinstance(param, list):\n",
        "            param = [param] * num_layers\n",
        "        return param"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "id": "09TJaoYAa7PJ",
        "outputId": "7b6b1b3e-2e45-49a7-ddbe-5bba042e677a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-236b998026b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGCNConv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGATConv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGraphConv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch_geometric/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimportlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimport_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch_geometric/data/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mhetero_data\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHeteroData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtemporal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTemporalData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch_geometric/data/data.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_sparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparseTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m from torch_geometric.data.feature_store import (\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch_sparse'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"https://raw.githubusercontent.com/QAZASDEDC/TopoMAD/master/DatasetUpdate/MBD%20(1).csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-toNbfHbDBA",
        "outputId": "97b92b1a-d26e-4bf4-dba3-205917ea8762"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py:3326: DtypeWarning: Columns (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dcG8Lk83chQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna()"
      ],
      "metadata": {
        "id": "23V0cLL1dWaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"label\"].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l81JTCSmda91",
        "outputId": "53dc1c1a-eb4e-484e-f5da-7bd99cb12142"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0    8074\n",
              "1.0     566\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in df[\"label\"]:\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "a_9Ki1-pdf7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://s3-us-west-2.amazonaws.com/telemanom/data.zip && unzip data.zip && rm data.zip\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPKPPDLPdrB6",
        "outputId": "8d744638-a30b-4f5d-e53c-86ad8c77cfb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-02 00:27:07--  https://s3-us-west-2.amazonaws.com/telemanom/data.zip\n",
            "Resolving s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)... 52.92.160.40, 52.92.212.24, 52.218.218.216, ...\n",
            "Connecting to s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)|52.92.160.40|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 85899803 (82M) [application/zip]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "data.zip            100%[===================>]  81.92M  15.9MB/s    in 6.5s    \n",
            "\n",
            "2023-02-02 00:27:14 (12.6 MB/s) - ‘data.zip’ saved [85899803/85899803]\n",
            "\n",
            "Archive:  data.zip\n",
            "   creating: data/2018-05-19_15.00.10/\n",
            "   creating: data/2018-05-19_15.00.10/models/\n",
            "  inflating: data/2018-05-19_15.00.10/models/A-1.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/A-2.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/A-3.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/A-4.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/A-5.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/A-6.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/A-7.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/A-8.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/A-9.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/B-1.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/C-1.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/C-2.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/D-1.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/D-11.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/D-12.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/D-13.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/D-14.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/D-15.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/D-16.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/D-2.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/D-3.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/D-4.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/D-5.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/D-6.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/D-7.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/D-8.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/D-9.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/E-1.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/E-10.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/E-11.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/E-12.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/E-13.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/E-2.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/E-3.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/E-4.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/E-5.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/E-6.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/E-7.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/E-8.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/E-9.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/F-1.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/F-2.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/F-3.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/F-4.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/F-5.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/F-7.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/F-8.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/G-1.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/G-2.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/G-3.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/G-4.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/G-6.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/G-7.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/M-1.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/M-2.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/M-3.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/M-4.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/M-5.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/M-6.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/M-7.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/P-1.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/P-10.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/P-11.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/P-14.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/P-15.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/P-2.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/P-3.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/P-4.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/P-7.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/R-1.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/S-1.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/S-2.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/T-1.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/T-10.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/T-12.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/T-13.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/T-2.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/T-3.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/T-4.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/T-5.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/T-8.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/models/T-9.h5  \n",
            "  inflating: data/2018-05-19_15.00.10/params.log  \n",
            "   creating: data/2018-05-19_15.00.10/smoothed_errors/\n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/A-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/A-2.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/A-3.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/A-4.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/A-5.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/A-6.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/A-7.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/A-8.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/A-9.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/B-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/C-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/C-2.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/D-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/D-11.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/D-12.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/D-13.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/D-14.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/D-15.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/D-16.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/D-2.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/D-3.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/D-4.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/D-5.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/D-6.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/D-7.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/D-8.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/D-9.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/E-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/E-10.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/E-11.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/E-12.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/E-13.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/E-2.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/E-3.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/E-4.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/E-5.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/E-6.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/E-7.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/E-8.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/E-9.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/F-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/F-2.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/F-3.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/F-4.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/F-5.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/F-7.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/F-8.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/G-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/G-2.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/G-3.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/G-4.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/G-6.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/G-7.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/M-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/M-2.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/M-3.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/M-4.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/M-5.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/M-6.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/M-7.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/P-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/P-10.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/P-11.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/P-14.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/P-15.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/P-2.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/P-3.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/P-4.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/P-7.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/R-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/S-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/S-2.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/T-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/T-10.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/T-12.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/T-13.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/T-2.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/T-3.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/T-4.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/T-5.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/T-8.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/smoothed_errors/T-9.npy  \n",
            "   creating: data/2018-05-19_15.00.10/y_hat/\n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/A-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/A-2.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/A-3.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/A-4.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/A-5.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/A-6.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/A-7.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/A-8.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/A-9.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/B-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/C-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/C-2.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/D-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/D-11.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/D-12.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/D-13.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/D-14.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/D-15.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/D-16.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/D-2.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/D-3.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/D-4.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/D-5.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/D-6.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/D-7.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/D-8.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/D-9.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/E-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/E-10.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/E-11.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/E-12.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/E-13.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/E-2.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/E-3.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/E-4.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/E-5.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/E-6.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/E-7.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/E-8.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/E-9.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/F-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/F-2.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/F-3.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/F-4.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/F-5.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/F-7.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/F-8.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/G-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/G-2.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/G-3.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/G-4.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/G-6.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/G-7.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/M-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/M-2.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/M-3.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/M-4.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/M-5.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/M-6.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/M-7.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/P-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/P-10.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/P-11.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/P-14.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/P-15.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/P-2.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/P-3.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/P-4.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/P-7.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/R-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/S-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/S-2.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/T-1.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/T-10.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/T-12.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/T-13.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/T-2.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/T-3.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/T-4.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/T-5.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/T-8.npy  \n",
            "  inflating: data/2018-05-19_15.00.10/y_hat/T-9.npy  \n",
            "   creating: data/test/\n",
            "  inflating: data/test/A-1.npy       \n",
            "  inflating: data/test/A-2.npy       \n",
            "  inflating: data/test/A-3.npy       \n",
            "  inflating: data/test/A-4.npy       \n",
            "  inflating: data/test/A-5.npy       \n",
            "  inflating: data/test/A-6.npy       \n",
            "  inflating: data/test/A-7.npy       \n",
            "  inflating: data/test/A-8.npy       \n",
            "  inflating: data/test/A-9.npy       \n",
            "  inflating: data/test/B-1.npy       \n",
            "  inflating: data/test/C-1.npy       \n",
            "  inflating: data/test/C-2.npy       \n",
            "  inflating: data/test/D-1.npy       \n",
            "  inflating: data/test/D-11.npy      \n",
            "  inflating: data/test/D-12.npy      \n",
            "  inflating: data/test/D-13.npy      \n",
            "  inflating: data/test/D-14.npy      \n",
            "  inflating: data/test/D-15.npy      \n",
            "  inflating: data/test/D-16.npy      \n",
            "  inflating: data/test/D-2.npy       \n",
            "  inflating: data/test/D-3.npy       \n",
            "  inflating: data/test/D-4.npy       \n",
            "  inflating: data/test/D-5.npy       \n",
            "  inflating: data/test/D-6.npy       \n",
            "  inflating: data/test/D-7.npy       \n",
            "  inflating: data/test/D-8.npy       \n",
            "  inflating: data/test/D-9.npy       \n",
            "  inflating: data/test/E-1.npy       \n",
            "  inflating: data/test/E-10.npy      \n",
            "  inflating: data/test/E-11.npy      \n",
            "  inflating: data/test/E-12.npy      \n",
            "  inflating: data/test/E-13.npy      \n",
            "  inflating: data/test/E-2.npy       \n",
            "  inflating: data/test/E-3.npy       \n",
            "  inflating: data/test/E-4.npy       \n",
            "  inflating: data/test/E-5.npy       \n",
            "  inflating: data/test/E-6.npy       \n",
            "  inflating: data/test/E-7.npy       \n",
            "  inflating: data/test/E-8.npy       \n",
            "  inflating: data/test/E-9.npy       \n",
            "  inflating: data/test/F-1.npy       \n",
            "  inflating: data/test/F-2.npy       \n",
            "  inflating: data/test/F-3.npy       \n",
            "  inflating: data/test/F-4.npy       \n",
            "  inflating: data/test/F-5.npy       \n",
            "  inflating: data/test/F-7.npy       \n",
            "  inflating: data/test/F-8.npy       \n",
            "  inflating: data/test/G-1.npy       \n",
            "  inflating: data/test/G-2.npy       \n",
            "  inflating: data/test/G-3.npy       \n",
            "  inflating: data/test/G-4.npy       \n",
            "  inflating: data/test/G-6.npy       \n",
            "  inflating: data/test/G-7.npy       \n",
            "  inflating: data/test/M-1.npy       \n",
            "  inflating: data/test/M-2.npy       \n",
            "  inflating: data/test/M-3.npy       \n",
            "  inflating: data/test/M-4.npy       \n",
            "  inflating: data/test/M-5.npy       \n",
            "  inflating: data/test/M-6.npy       \n",
            "  inflating: data/test/M-7.npy       \n",
            "  inflating: data/test/P-1.npy       \n",
            "  inflating: data/test/P-10.npy      \n",
            "  inflating: data/test/P-11.npy      \n",
            "  inflating: data/test/P-14.npy      \n",
            "  inflating: data/test/P-15.npy      \n",
            "  inflating: data/test/P-2.npy       \n",
            "  inflating: data/test/P-3.npy       \n",
            "  inflating: data/test/P-4.npy       \n",
            "  inflating: data/test/P-7.npy       \n",
            "  inflating: data/test/R-1.npy       \n",
            "  inflating: data/test/S-1.npy       \n",
            "  inflating: data/test/S-2.npy       \n",
            "  inflating: data/test/T-1.npy       \n",
            "  inflating: data/test/T-10.npy      \n",
            "  inflating: data/test/T-12.npy      \n",
            "  inflating: data/test/T-13.npy      \n",
            "  inflating: data/test/T-2.npy       \n",
            "  inflating: data/test/T-3.npy       \n",
            "  inflating: data/test/T-4.npy       \n",
            "  inflating: data/test/T-5.npy       \n",
            "  inflating: data/test/T-8.npy       \n",
            "  inflating: data/test/T-9.npy       \n",
            "   creating: data/train/\n",
            "  inflating: data/train/A-1.npy      \n",
            "  inflating: data/train/A-2.npy      \n",
            "  inflating: data/train/A-3.npy      \n",
            "  inflating: data/train/A-4.npy      \n",
            "  inflating: data/train/A-5.npy      \n",
            "  inflating: data/train/A-6.npy      \n",
            "  inflating: data/train/A-7.npy      \n",
            "  inflating: data/train/A-8.npy      \n",
            "  inflating: data/train/A-9.npy      \n",
            "  inflating: data/train/B-1.npy      \n",
            "  inflating: data/train/C-1.npy      \n",
            "  inflating: data/train/C-2.npy      \n",
            "  inflating: data/train/D-1.npy      \n",
            "  inflating: data/train/D-11.npy     \n",
            "  inflating: data/train/D-12.npy     \n",
            "  inflating: data/train/D-13.npy     \n",
            "  inflating: data/train/D-14.npy     \n",
            "  inflating: data/train/D-15.npy     \n",
            "  inflating: data/train/D-16.npy     \n",
            "  inflating: data/train/D-2.npy      \n",
            "  inflating: data/train/D-3.npy      \n",
            "  inflating: data/train/D-4.npy      \n",
            "  inflating: data/train/D-5.npy      \n",
            "  inflating: data/train/D-6.npy      \n",
            "  inflating: data/train/D-7.npy      \n",
            "  inflating: data/train/D-8.npy      \n",
            "  inflating: data/train/D-9.npy      \n",
            "  inflating: data/train/E-1.npy      \n",
            "  inflating: data/train/E-10.npy     \n",
            "  inflating: data/train/E-11.npy     \n",
            "  inflating: data/train/E-12.npy     \n",
            "  inflating: data/train/E-13.npy     \n",
            "  inflating: data/train/E-2.npy      \n",
            "  inflating: data/train/E-3.npy      \n",
            "  inflating: data/train/E-4.npy      \n",
            "  inflating: data/train/E-5.npy      \n",
            "  inflating: data/train/E-6.npy      \n",
            "  inflating: data/train/E-7.npy      \n",
            "  inflating: data/train/E-8.npy      \n",
            "  inflating: data/train/E-9.npy      \n",
            "  inflating: data/train/F-1.npy      \n",
            "  inflating: data/train/F-2.npy      \n",
            "  inflating: data/train/F-3.npy      \n",
            "  inflating: data/train/F-4.npy      \n",
            "  inflating: data/train/F-5.npy      \n",
            "  inflating: data/train/F-7.npy      \n",
            "  inflating: data/train/F-8.npy      \n",
            "  inflating: data/train/G-1.npy      \n",
            "  inflating: data/train/G-2.npy      \n",
            "  inflating: data/train/G-3.npy      \n",
            "  inflating: data/train/G-4.npy      \n",
            "  inflating: data/train/G-6.npy      \n",
            "  inflating: data/train/G-7.npy      \n",
            "  inflating: data/train/M-1.npy      \n",
            "  inflating: data/train/M-2.npy      \n",
            "  inflating: data/train/M-3.npy      \n",
            "  inflating: data/train/M-4.npy      \n",
            "  inflating: data/train/M-5.npy      \n",
            "  inflating: data/train/M-6.npy      \n",
            "  inflating: data/train/M-7.npy      \n",
            "  inflating: data/train/P-1.npy      \n",
            "  inflating: data/train/P-10.npy     \n",
            "  inflating: data/train/P-11.npy     \n",
            "  inflating: data/train/P-14.npy     \n",
            "  inflating: data/train/P-15.npy     \n",
            "  inflating: data/train/P-2.npy      \n",
            "  inflating: data/train/P-3.npy      \n",
            "  inflating: data/train/P-4.npy      \n",
            "  inflating: data/train/P-7.npy      \n",
            "  inflating: data/train/R-1.npy      \n",
            "  inflating: data/train/S-1.npy      \n",
            "  inflating: data/train/S-2.npy      \n",
            "  inflating: data/train/T-1.npy      \n",
            "  inflating: data/train/T-10.npy     \n",
            "  inflating: data/train/T-12.npy     \n",
            "  inflating: data/train/T-13.npy     \n",
            "  inflating: data/train/T-2.npy      \n",
            "  inflating: data/train/T-3.npy      \n",
            "  inflating: data/train/T-4.npy      \n",
            "  inflating: data/train/T-5.npy      \n",
            "  inflating: data/train/T-8.npy      \n",
            "  inflating: data/train/T-9.npy      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install  ts_datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1MD_Er4hRsL",
        "outputId": "9544f46a-f4c0-4618-d883-41a8357a75a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement ts_datasets (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for ts_datasets\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import scipy.io as sio\n",
        "test = sio.loadmat('/content/shuttle.mat')"
      ],
      "metadata": {
        "id": "QigcBYCPh73i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = test[\"y\"]"
      ],
      "metadata": {
        "id": "lYUe-WC4jtWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = test[\"X\"]"
      ],
      "metadata": {
        "id": "C3y2tzpIjuGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = test[\"header\"]"
      ],
      "metadata": {
        "id": "MCtXm2sYkAL8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "673cb428-ded8-43d8-e304-f4cd02ff1fd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-3bad91e007a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m: 'header'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test['X']\n",
        "# load .mat file into python pandas dataframe\n",
        "def loadmat(filename):\n",
        "    data = sio.loadmat(filename)\n",
        "    return pd.DataFrame(data['X'])"
      ],
      "metadata": {
        "id": "Hd79Njnfl7ds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = loadmat('/content/shuttle.mat')"
      ],
      "metadata": {
        "id": "TzapUdcWl_9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "id": "hswyzW7amO3C",
        "outputId": "b63751eb-fb6a-48a9-8c5d-86da5307d051",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        0   1    2  3   4   5   6    7    8\n",
              "0      50  21   77  0  28   0  27   48   22\n",
              "1      53   0   82  0  52  -5  29   30    2\n",
              "2      37   0   76  0  28  18  40   48    8\n",
              "3      37   0   79  0  34 -26  43   46    2\n",
              "4      85   0   88 -4   6   1   3   83   80\n",
              "...    ..  ..  ... ..  ..  ..  ..  ...  ...\n",
              "49092  39  -2   80 -4  38   0  41   41    0\n",
              "49093  43   0   81  1  42  -9  37   39    2\n",
              "49094  49   0   87  0  46 -12  38   41    2\n",
              "49095  80   0   84  0 -36 -29   4  120  116\n",
              "49096  37   0  103  0  18 -16  66   85   20\n",
              "\n",
              "[49097 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-250fd416-e509-4c96-81e7-a56a4a647cae\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>50</td>\n",
              "      <td>21</td>\n",
              "      <td>77</td>\n",
              "      <td>0</td>\n",
              "      <td>28</td>\n",
              "      <td>0</td>\n",
              "      <td>27</td>\n",
              "      <td>48</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>53</td>\n",
              "      <td>0</td>\n",
              "      <td>82</td>\n",
              "      <td>0</td>\n",
              "      <td>52</td>\n",
              "      <td>-5</td>\n",
              "      <td>29</td>\n",
              "      <td>30</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>37</td>\n",
              "      <td>0</td>\n",
              "      <td>76</td>\n",
              "      <td>0</td>\n",
              "      <td>28</td>\n",
              "      <td>18</td>\n",
              "      <td>40</td>\n",
              "      <td>48</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>37</td>\n",
              "      <td>0</td>\n",
              "      <td>79</td>\n",
              "      <td>0</td>\n",
              "      <td>34</td>\n",
              "      <td>-26</td>\n",
              "      <td>43</td>\n",
              "      <td>46</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>85</td>\n",
              "      <td>0</td>\n",
              "      <td>88</td>\n",
              "      <td>-4</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>83</td>\n",
              "      <td>80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49092</th>\n",
              "      <td>39</td>\n",
              "      <td>-2</td>\n",
              "      <td>80</td>\n",
              "      <td>-4</td>\n",
              "      <td>38</td>\n",
              "      <td>0</td>\n",
              "      <td>41</td>\n",
              "      <td>41</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49093</th>\n",
              "      <td>43</td>\n",
              "      <td>0</td>\n",
              "      <td>81</td>\n",
              "      <td>1</td>\n",
              "      <td>42</td>\n",
              "      <td>-9</td>\n",
              "      <td>37</td>\n",
              "      <td>39</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49094</th>\n",
              "      <td>49</td>\n",
              "      <td>0</td>\n",
              "      <td>87</td>\n",
              "      <td>0</td>\n",
              "      <td>46</td>\n",
              "      <td>-12</td>\n",
              "      <td>38</td>\n",
              "      <td>41</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49095</th>\n",
              "      <td>80</td>\n",
              "      <td>0</td>\n",
              "      <td>84</td>\n",
              "      <td>0</td>\n",
              "      <td>-36</td>\n",
              "      <td>-29</td>\n",
              "      <td>4</td>\n",
              "      <td>120</td>\n",
              "      <td>116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49096</th>\n",
              "      <td>37</td>\n",
              "      <td>0</td>\n",
              "      <td>103</td>\n",
              "      <td>0</td>\n",
              "      <td>18</td>\n",
              "      <td>-16</td>\n",
              "      <td>66</td>\n",
              "      <td>85</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>49097 rows × 9 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-250fd416-e509-4c96-81e7-a56a4a647cae')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-250fd416-e509-4c96-81e7-a56a4a647cae button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-250fd416-e509-4c96-81e7-a56a4a647cae');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd \n",
        "x = pd.read_csv(\"/content/ECG5000_TRAIN.txt\",header=None,sep = \"\")"
      ],
      "metadata": {
        "id": "oqWQXg4DmPbA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "outputId": "1a0d3616-7c06-41ab-d534-a44aff59c076"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ParserError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-24e357919895>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/ECG5000_TRAIN.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m         \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 360 fields in line 2, saw 378\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the dataset\n",
        "dataframe = pd.read_csv('http://storage.googleapis.com/download.tensorflow.org/data/ecg.csv', header=None)\n",
        "raw_data = dataframe.values\n",
        "X_train = dataframe[0:500]\n",
        "X_train = dataframe[0:500]\n",
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "id": "1uI-XRo4nZbi",
        "outputId": "8b9733b7-aa73-488a-8e09-db1872872ac4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           0         1         2         3         4         5         6    \\\n",
              "0    -0.112522 -2.827204 -3.773897 -4.349751 -4.376041 -3.474986 -2.181408   \n",
              "1    -1.100878 -3.996840 -4.285843 -4.506579 -4.022377 -3.234368 -1.566126   \n",
              "2    -0.567088 -2.593450 -3.874230 -4.584095 -4.187449 -3.151462 -1.742940   \n",
              "3     0.490473 -1.914407 -3.616364 -4.318823 -4.268016 -3.881110 -2.993280   \n",
              "4     0.800232 -0.874252 -2.384761 -3.973292 -4.338224 -3.802422 -2.534510   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "4993  0.608558 -0.335651 -0.990948 -1.784153 -2.626145 -2.957065 -2.931897   \n",
              "4994 -2.060402 -2.860116 -3.405074 -3.748719 -3.513561 -3.006545 -2.234850   \n",
              "4995 -1.122969 -2.252925 -2.867628 -3.358605 -3.167849 -2.638360 -1.664162   \n",
              "4996 -0.547705 -1.889545 -2.839779 -3.457912 -3.929149 -3.966026 -3.492560   \n",
              "4997 -1.351779 -2.209006 -2.520225 -3.061475 -3.065141 -3.030739 -2.622720   \n",
              "\n",
              "           7         8         9    ...       131       132       133  \\\n",
              "0    -1.818286 -1.250522 -0.477492  ...  0.792168  0.933541  0.796958   \n",
              "1    -0.992258 -0.754680  0.042321  ...  0.538356  0.656881  0.787490   \n",
              "2    -1.490659 -1.183580 -0.394229  ...  0.886073  0.531452  0.311377   \n",
              "3    -1.671131 -1.333884 -0.965629  ...  0.350816  0.499111  0.600345   \n",
              "4    -1.783423 -1.594450 -0.753199  ...  1.148884  0.958434  1.059025   \n",
              "...        ...       ...       ...  ...       ...       ...       ...   \n",
              "4993 -2.664816 -2.090137 -1.461841  ...  1.757705  2.291923  2.704595   \n",
              "4994 -1.593270 -1.075279 -0.976047  ...  1.388947  2.079675  2.433375   \n",
              "4995 -0.935655 -0.866953 -0.645363  ... -0.472419 -1.310147 -2.029521   \n",
              "4996 -2.695270 -1.849691 -1.374321  ...  1.258419  1.907530  2.280888   \n",
              "4997 -2.044092 -1.295874 -0.733839  ... -1.512234 -2.076075 -2.586042   \n",
              "\n",
              "           134       135       136       137       138       139  140  \n",
              "0     0.578621  0.257740  0.228077  0.123431  0.925286  0.193137  1.0  \n",
              "1     0.724046  0.555784  0.476333  0.773820  1.119621 -1.436250  1.0  \n",
              "2    -0.021919 -0.713683 -0.532197  0.321097  0.904227 -0.421797  1.0  \n",
              "3     0.842069  0.952074  0.990133  1.086798  1.403011 -0.383564  1.0  \n",
              "4     1.371682  1.277392  0.960304  0.971020  1.614392  1.421456  1.0  \n",
              "...        ...       ...       ...       ...       ...       ...  ...  \n",
              "4993  2.451519  2.017396  1.704358  1.688542  1.629593  1.342651  0.0  \n",
              "4994  2.159484  1.819747  1.534767  1.696818  1.483832  1.047612  0.0  \n",
              "4995 -3.221294 -4.176790 -4.009720 -2.874136 -2.008369 -1.808334  0.0  \n",
              "4996  1.895242  1.437702  1.193433  1.261335  1.150449  0.804932  0.0  \n",
              "4997 -3.322799 -3.627311 -3.437038 -2.260023 -1.577823 -0.684531  0.0  \n",
              "\n",
              "[4998 rows x 141 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-975f53e4-7bce-435c-9628-4e47fa7656e7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>131</th>\n",
              "      <th>132</th>\n",
              "      <th>133</th>\n",
              "      <th>134</th>\n",
              "      <th>135</th>\n",
              "      <th>136</th>\n",
              "      <th>137</th>\n",
              "      <th>138</th>\n",
              "      <th>139</th>\n",
              "      <th>140</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.112522</td>\n",
              "      <td>-2.827204</td>\n",
              "      <td>-3.773897</td>\n",
              "      <td>-4.349751</td>\n",
              "      <td>-4.376041</td>\n",
              "      <td>-3.474986</td>\n",
              "      <td>-2.181408</td>\n",
              "      <td>-1.818286</td>\n",
              "      <td>-1.250522</td>\n",
              "      <td>-0.477492</td>\n",
              "      <td>...</td>\n",
              "      <td>0.792168</td>\n",
              "      <td>0.933541</td>\n",
              "      <td>0.796958</td>\n",
              "      <td>0.578621</td>\n",
              "      <td>0.257740</td>\n",
              "      <td>0.228077</td>\n",
              "      <td>0.123431</td>\n",
              "      <td>0.925286</td>\n",
              "      <td>0.193137</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1.100878</td>\n",
              "      <td>-3.996840</td>\n",
              "      <td>-4.285843</td>\n",
              "      <td>-4.506579</td>\n",
              "      <td>-4.022377</td>\n",
              "      <td>-3.234368</td>\n",
              "      <td>-1.566126</td>\n",
              "      <td>-0.992258</td>\n",
              "      <td>-0.754680</td>\n",
              "      <td>0.042321</td>\n",
              "      <td>...</td>\n",
              "      <td>0.538356</td>\n",
              "      <td>0.656881</td>\n",
              "      <td>0.787490</td>\n",
              "      <td>0.724046</td>\n",
              "      <td>0.555784</td>\n",
              "      <td>0.476333</td>\n",
              "      <td>0.773820</td>\n",
              "      <td>1.119621</td>\n",
              "      <td>-1.436250</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.567088</td>\n",
              "      <td>-2.593450</td>\n",
              "      <td>-3.874230</td>\n",
              "      <td>-4.584095</td>\n",
              "      <td>-4.187449</td>\n",
              "      <td>-3.151462</td>\n",
              "      <td>-1.742940</td>\n",
              "      <td>-1.490659</td>\n",
              "      <td>-1.183580</td>\n",
              "      <td>-0.394229</td>\n",
              "      <td>...</td>\n",
              "      <td>0.886073</td>\n",
              "      <td>0.531452</td>\n",
              "      <td>0.311377</td>\n",
              "      <td>-0.021919</td>\n",
              "      <td>-0.713683</td>\n",
              "      <td>-0.532197</td>\n",
              "      <td>0.321097</td>\n",
              "      <td>0.904227</td>\n",
              "      <td>-0.421797</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.490473</td>\n",
              "      <td>-1.914407</td>\n",
              "      <td>-3.616364</td>\n",
              "      <td>-4.318823</td>\n",
              "      <td>-4.268016</td>\n",
              "      <td>-3.881110</td>\n",
              "      <td>-2.993280</td>\n",
              "      <td>-1.671131</td>\n",
              "      <td>-1.333884</td>\n",
              "      <td>-0.965629</td>\n",
              "      <td>...</td>\n",
              "      <td>0.350816</td>\n",
              "      <td>0.499111</td>\n",
              "      <td>0.600345</td>\n",
              "      <td>0.842069</td>\n",
              "      <td>0.952074</td>\n",
              "      <td>0.990133</td>\n",
              "      <td>1.086798</td>\n",
              "      <td>1.403011</td>\n",
              "      <td>-0.383564</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.800232</td>\n",
              "      <td>-0.874252</td>\n",
              "      <td>-2.384761</td>\n",
              "      <td>-3.973292</td>\n",
              "      <td>-4.338224</td>\n",
              "      <td>-3.802422</td>\n",
              "      <td>-2.534510</td>\n",
              "      <td>-1.783423</td>\n",
              "      <td>-1.594450</td>\n",
              "      <td>-0.753199</td>\n",
              "      <td>...</td>\n",
              "      <td>1.148884</td>\n",
              "      <td>0.958434</td>\n",
              "      <td>1.059025</td>\n",
              "      <td>1.371682</td>\n",
              "      <td>1.277392</td>\n",
              "      <td>0.960304</td>\n",
              "      <td>0.971020</td>\n",
              "      <td>1.614392</td>\n",
              "      <td>1.421456</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4993</th>\n",
              "      <td>0.608558</td>\n",
              "      <td>-0.335651</td>\n",
              "      <td>-0.990948</td>\n",
              "      <td>-1.784153</td>\n",
              "      <td>-2.626145</td>\n",
              "      <td>-2.957065</td>\n",
              "      <td>-2.931897</td>\n",
              "      <td>-2.664816</td>\n",
              "      <td>-2.090137</td>\n",
              "      <td>-1.461841</td>\n",
              "      <td>...</td>\n",
              "      <td>1.757705</td>\n",
              "      <td>2.291923</td>\n",
              "      <td>2.704595</td>\n",
              "      <td>2.451519</td>\n",
              "      <td>2.017396</td>\n",
              "      <td>1.704358</td>\n",
              "      <td>1.688542</td>\n",
              "      <td>1.629593</td>\n",
              "      <td>1.342651</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4994</th>\n",
              "      <td>-2.060402</td>\n",
              "      <td>-2.860116</td>\n",
              "      <td>-3.405074</td>\n",
              "      <td>-3.748719</td>\n",
              "      <td>-3.513561</td>\n",
              "      <td>-3.006545</td>\n",
              "      <td>-2.234850</td>\n",
              "      <td>-1.593270</td>\n",
              "      <td>-1.075279</td>\n",
              "      <td>-0.976047</td>\n",
              "      <td>...</td>\n",
              "      <td>1.388947</td>\n",
              "      <td>2.079675</td>\n",
              "      <td>2.433375</td>\n",
              "      <td>2.159484</td>\n",
              "      <td>1.819747</td>\n",
              "      <td>1.534767</td>\n",
              "      <td>1.696818</td>\n",
              "      <td>1.483832</td>\n",
              "      <td>1.047612</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4995</th>\n",
              "      <td>-1.122969</td>\n",
              "      <td>-2.252925</td>\n",
              "      <td>-2.867628</td>\n",
              "      <td>-3.358605</td>\n",
              "      <td>-3.167849</td>\n",
              "      <td>-2.638360</td>\n",
              "      <td>-1.664162</td>\n",
              "      <td>-0.935655</td>\n",
              "      <td>-0.866953</td>\n",
              "      <td>-0.645363</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.472419</td>\n",
              "      <td>-1.310147</td>\n",
              "      <td>-2.029521</td>\n",
              "      <td>-3.221294</td>\n",
              "      <td>-4.176790</td>\n",
              "      <td>-4.009720</td>\n",
              "      <td>-2.874136</td>\n",
              "      <td>-2.008369</td>\n",
              "      <td>-1.808334</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4996</th>\n",
              "      <td>-0.547705</td>\n",
              "      <td>-1.889545</td>\n",
              "      <td>-2.839779</td>\n",
              "      <td>-3.457912</td>\n",
              "      <td>-3.929149</td>\n",
              "      <td>-3.966026</td>\n",
              "      <td>-3.492560</td>\n",
              "      <td>-2.695270</td>\n",
              "      <td>-1.849691</td>\n",
              "      <td>-1.374321</td>\n",
              "      <td>...</td>\n",
              "      <td>1.258419</td>\n",
              "      <td>1.907530</td>\n",
              "      <td>2.280888</td>\n",
              "      <td>1.895242</td>\n",
              "      <td>1.437702</td>\n",
              "      <td>1.193433</td>\n",
              "      <td>1.261335</td>\n",
              "      <td>1.150449</td>\n",
              "      <td>0.804932</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4997</th>\n",
              "      <td>-1.351779</td>\n",
              "      <td>-2.209006</td>\n",
              "      <td>-2.520225</td>\n",
              "      <td>-3.061475</td>\n",
              "      <td>-3.065141</td>\n",
              "      <td>-3.030739</td>\n",
              "      <td>-2.622720</td>\n",
              "      <td>-2.044092</td>\n",
              "      <td>-1.295874</td>\n",
              "      <td>-0.733839</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.512234</td>\n",
              "      <td>-2.076075</td>\n",
              "      <td>-2.586042</td>\n",
              "      <td>-3.322799</td>\n",
              "      <td>-3.627311</td>\n",
              "      <td>-3.437038</td>\n",
              "      <td>-2.260023</td>\n",
              "      <td>-1.577823</td>\n",
              "      <td>-0.684531</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4998 rows × 141 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-975f53e4-7bce-435c-9628-4e47fa7656e7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-975f53e4-7bce-435c-9628-4e47fa7656e7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-975f53e4-7bce-435c-9628-4e47fa7656e7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_data, test_data, train_labels, test_labels = train_test_split(\n",
        "    dataframe, dataframe[140], test_size=0.1, random_state=21,shuffle = False\n",
        ")"
      ],
      "metadata": {
        "id": "0zXGGEiknmtl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The last element contains the labels\n",
        "labels = raw_data[:, -1]\n",
        "\n",
        "# The other data points are the electrocadriogram data\n",
        "data = raw_data[:, 0:-1]\n",
        "\n",
        "train_data, test_data, train_labels, test_labels = train_test_split(\n",
        "    data, labels, test_size=0.9, random_state=21,shuffle = False\n",
        ")"
      ],
      "metadata": {
        "id": "fH62Ndzbo9up"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.DataFrame(train_data)"
      ],
      "metadata": {
        "id": "4ac9-3ybpBI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = dataframe[dataframe[140]!=0]"
      ],
      "metadata": {
        "id": "u-KSVNnGpLEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "id": "-v4SvlzepSI6",
        "outputId": "c6e41cd6-5224-4d23-e932-5d29b4f62da9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           0         1         2         3         4         5         6    \\\n",
              "0    -0.112522 -2.827204 -3.773897 -4.349751 -4.376041 -3.474986 -2.181408   \n",
              "1    -1.100878 -3.996840 -4.285843 -4.506579 -4.022377 -3.234368 -1.566126   \n",
              "2    -0.567088 -2.593450 -3.874230 -4.584095 -4.187449 -3.151462 -1.742940   \n",
              "3     0.490473 -1.914407 -3.616364 -4.318823 -4.268016 -3.881110 -2.993280   \n",
              "4     0.800232 -0.874252 -2.384761 -3.973292 -4.338224 -3.802422 -2.534510   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "3122 -3.107567 -4.521302 -3.976050 -3.930939 -3.210632 -2.047815 -1.442905   \n",
              "3123 -1.693958 -3.318677 -3.784877 -4.073614 -3.864253 -3.347139 -2.322313   \n",
              "3124 -2.089921 -2.982796 -3.850578 -3.861497 -3.039973 -1.381897 -0.621009   \n",
              "3125 -2.123972 -3.247339 -3.737318 -4.037332 -3.763836 -3.070836 -1.763453   \n",
              "3126 -1.553834 -4.338159 -4.225774 -4.418764 -3.302098 -2.103476 -1.711355   \n",
              "\n",
              "           7         8         9    ...       131       132       133  \\\n",
              "0    -1.818286 -1.250522 -0.477492  ...  0.792168  0.933541  0.796958   \n",
              "1    -0.992258 -0.754680  0.042321  ...  0.538356  0.656881  0.787490   \n",
              "2    -1.490659 -1.183580 -0.394229  ...  0.886073  0.531452  0.311377   \n",
              "3    -1.671131 -1.333884 -0.965629  ...  0.350816  0.499111  0.600345   \n",
              "4    -1.783423 -1.594450 -0.753199  ...  1.148884  0.958434  1.059025   \n",
              "...        ...       ...       ...  ...       ...       ...       ...   \n",
              "3122 -1.271809 -0.809131 -0.464601  ...  1.278968  1.389388  1.464593   \n",
              "3123 -1.650408 -1.454390 -0.877257  ...  1.234095  1.209287  1.166618   \n",
              "3124  0.114544  1.033027  1.309473  ...  0.369837  0.623082  1.039331   \n",
              "3125 -1.400594 -1.026897 -0.340909  ...  0.486942  0.442708  0.167527   \n",
              "3126 -1.369171 -0.610783 -0.381563  ...  1.285824  1.568415  1.360514   \n",
              "\n",
              "           134       135       136       137       138       139  140  \n",
              "0     0.578621  0.257740  0.228077  0.123431  0.925286  0.193137  1.0  \n",
              "1     0.724046  0.555784  0.476333  0.773820  1.119621 -1.436250  1.0  \n",
              "2    -0.021919 -0.713683 -0.532197  0.321097  0.904227 -0.421797  1.0  \n",
              "3     0.842069  0.952074  0.990133  1.086798  1.403011 -0.383564  1.0  \n",
              "4     1.371682  1.277392  0.960304  0.971020  1.614392  1.421456  1.0  \n",
              "...        ...       ...       ...       ...       ...       ...  ...  \n",
              "3122  1.400539  1.125004  1.029697  0.998533  1.075872 -0.533409  1.0  \n",
              "3123  0.903829  0.637425  0.944447  0.827138 -0.182124 -1.924306  1.0  \n",
              "3124  1.016398  0.299646 -0.971958 -1.783307 -2.003871 -3.500333  1.0  \n",
              "3125  0.121955 -0.224867 -0.298590 -0.194366 -1.001879 -2.918817  1.0  \n",
              "3126  1.096972  0.405206  0.051660  0.139288  1.081321 -1.221625  1.0  \n",
              "\n",
              "[2919 rows x 141 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-075fe436-7077-46fc-96c3-f51a9be028a9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>131</th>\n",
              "      <th>132</th>\n",
              "      <th>133</th>\n",
              "      <th>134</th>\n",
              "      <th>135</th>\n",
              "      <th>136</th>\n",
              "      <th>137</th>\n",
              "      <th>138</th>\n",
              "      <th>139</th>\n",
              "      <th>140</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.112522</td>\n",
              "      <td>-2.827204</td>\n",
              "      <td>-3.773897</td>\n",
              "      <td>-4.349751</td>\n",
              "      <td>-4.376041</td>\n",
              "      <td>-3.474986</td>\n",
              "      <td>-2.181408</td>\n",
              "      <td>-1.818286</td>\n",
              "      <td>-1.250522</td>\n",
              "      <td>-0.477492</td>\n",
              "      <td>...</td>\n",
              "      <td>0.792168</td>\n",
              "      <td>0.933541</td>\n",
              "      <td>0.796958</td>\n",
              "      <td>0.578621</td>\n",
              "      <td>0.257740</td>\n",
              "      <td>0.228077</td>\n",
              "      <td>0.123431</td>\n",
              "      <td>0.925286</td>\n",
              "      <td>0.193137</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1.100878</td>\n",
              "      <td>-3.996840</td>\n",
              "      <td>-4.285843</td>\n",
              "      <td>-4.506579</td>\n",
              "      <td>-4.022377</td>\n",
              "      <td>-3.234368</td>\n",
              "      <td>-1.566126</td>\n",
              "      <td>-0.992258</td>\n",
              "      <td>-0.754680</td>\n",
              "      <td>0.042321</td>\n",
              "      <td>...</td>\n",
              "      <td>0.538356</td>\n",
              "      <td>0.656881</td>\n",
              "      <td>0.787490</td>\n",
              "      <td>0.724046</td>\n",
              "      <td>0.555784</td>\n",
              "      <td>0.476333</td>\n",
              "      <td>0.773820</td>\n",
              "      <td>1.119621</td>\n",
              "      <td>-1.436250</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.567088</td>\n",
              "      <td>-2.593450</td>\n",
              "      <td>-3.874230</td>\n",
              "      <td>-4.584095</td>\n",
              "      <td>-4.187449</td>\n",
              "      <td>-3.151462</td>\n",
              "      <td>-1.742940</td>\n",
              "      <td>-1.490659</td>\n",
              "      <td>-1.183580</td>\n",
              "      <td>-0.394229</td>\n",
              "      <td>...</td>\n",
              "      <td>0.886073</td>\n",
              "      <td>0.531452</td>\n",
              "      <td>0.311377</td>\n",
              "      <td>-0.021919</td>\n",
              "      <td>-0.713683</td>\n",
              "      <td>-0.532197</td>\n",
              "      <td>0.321097</td>\n",
              "      <td>0.904227</td>\n",
              "      <td>-0.421797</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.490473</td>\n",
              "      <td>-1.914407</td>\n",
              "      <td>-3.616364</td>\n",
              "      <td>-4.318823</td>\n",
              "      <td>-4.268016</td>\n",
              "      <td>-3.881110</td>\n",
              "      <td>-2.993280</td>\n",
              "      <td>-1.671131</td>\n",
              "      <td>-1.333884</td>\n",
              "      <td>-0.965629</td>\n",
              "      <td>...</td>\n",
              "      <td>0.350816</td>\n",
              "      <td>0.499111</td>\n",
              "      <td>0.600345</td>\n",
              "      <td>0.842069</td>\n",
              "      <td>0.952074</td>\n",
              "      <td>0.990133</td>\n",
              "      <td>1.086798</td>\n",
              "      <td>1.403011</td>\n",
              "      <td>-0.383564</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.800232</td>\n",
              "      <td>-0.874252</td>\n",
              "      <td>-2.384761</td>\n",
              "      <td>-3.973292</td>\n",
              "      <td>-4.338224</td>\n",
              "      <td>-3.802422</td>\n",
              "      <td>-2.534510</td>\n",
              "      <td>-1.783423</td>\n",
              "      <td>-1.594450</td>\n",
              "      <td>-0.753199</td>\n",
              "      <td>...</td>\n",
              "      <td>1.148884</td>\n",
              "      <td>0.958434</td>\n",
              "      <td>1.059025</td>\n",
              "      <td>1.371682</td>\n",
              "      <td>1.277392</td>\n",
              "      <td>0.960304</td>\n",
              "      <td>0.971020</td>\n",
              "      <td>1.614392</td>\n",
              "      <td>1.421456</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3122</th>\n",
              "      <td>-3.107567</td>\n",
              "      <td>-4.521302</td>\n",
              "      <td>-3.976050</td>\n",
              "      <td>-3.930939</td>\n",
              "      <td>-3.210632</td>\n",
              "      <td>-2.047815</td>\n",
              "      <td>-1.442905</td>\n",
              "      <td>-1.271809</td>\n",
              "      <td>-0.809131</td>\n",
              "      <td>-0.464601</td>\n",
              "      <td>...</td>\n",
              "      <td>1.278968</td>\n",
              "      <td>1.389388</td>\n",
              "      <td>1.464593</td>\n",
              "      <td>1.400539</td>\n",
              "      <td>1.125004</td>\n",
              "      <td>1.029697</td>\n",
              "      <td>0.998533</td>\n",
              "      <td>1.075872</td>\n",
              "      <td>-0.533409</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3123</th>\n",
              "      <td>-1.693958</td>\n",
              "      <td>-3.318677</td>\n",
              "      <td>-3.784877</td>\n",
              "      <td>-4.073614</td>\n",
              "      <td>-3.864253</td>\n",
              "      <td>-3.347139</td>\n",
              "      <td>-2.322313</td>\n",
              "      <td>-1.650408</td>\n",
              "      <td>-1.454390</td>\n",
              "      <td>-0.877257</td>\n",
              "      <td>...</td>\n",
              "      <td>1.234095</td>\n",
              "      <td>1.209287</td>\n",
              "      <td>1.166618</td>\n",
              "      <td>0.903829</td>\n",
              "      <td>0.637425</td>\n",
              "      <td>0.944447</td>\n",
              "      <td>0.827138</td>\n",
              "      <td>-0.182124</td>\n",
              "      <td>-1.924306</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3124</th>\n",
              "      <td>-2.089921</td>\n",
              "      <td>-2.982796</td>\n",
              "      <td>-3.850578</td>\n",
              "      <td>-3.861497</td>\n",
              "      <td>-3.039973</td>\n",
              "      <td>-1.381897</td>\n",
              "      <td>-0.621009</td>\n",
              "      <td>0.114544</td>\n",
              "      <td>1.033027</td>\n",
              "      <td>1.309473</td>\n",
              "      <td>...</td>\n",
              "      <td>0.369837</td>\n",
              "      <td>0.623082</td>\n",
              "      <td>1.039331</td>\n",
              "      <td>1.016398</td>\n",
              "      <td>0.299646</td>\n",
              "      <td>-0.971958</td>\n",
              "      <td>-1.783307</td>\n",
              "      <td>-2.003871</td>\n",
              "      <td>-3.500333</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3125</th>\n",
              "      <td>-2.123972</td>\n",
              "      <td>-3.247339</td>\n",
              "      <td>-3.737318</td>\n",
              "      <td>-4.037332</td>\n",
              "      <td>-3.763836</td>\n",
              "      <td>-3.070836</td>\n",
              "      <td>-1.763453</td>\n",
              "      <td>-1.400594</td>\n",
              "      <td>-1.026897</td>\n",
              "      <td>-0.340909</td>\n",
              "      <td>...</td>\n",
              "      <td>0.486942</td>\n",
              "      <td>0.442708</td>\n",
              "      <td>0.167527</td>\n",
              "      <td>0.121955</td>\n",
              "      <td>-0.224867</td>\n",
              "      <td>-0.298590</td>\n",
              "      <td>-0.194366</td>\n",
              "      <td>-1.001879</td>\n",
              "      <td>-2.918817</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3126</th>\n",
              "      <td>-1.553834</td>\n",
              "      <td>-4.338159</td>\n",
              "      <td>-4.225774</td>\n",
              "      <td>-4.418764</td>\n",
              "      <td>-3.302098</td>\n",
              "      <td>-2.103476</td>\n",
              "      <td>-1.711355</td>\n",
              "      <td>-1.369171</td>\n",
              "      <td>-0.610783</td>\n",
              "      <td>-0.381563</td>\n",
              "      <td>...</td>\n",
              "      <td>1.285824</td>\n",
              "      <td>1.568415</td>\n",
              "      <td>1.360514</td>\n",
              "      <td>1.096972</td>\n",
              "      <td>0.405206</td>\n",
              "      <td>0.051660</td>\n",
              "      <td>0.139288</td>\n",
              "      <td>1.081321</td>\n",
              "      <td>-1.221625</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2919 rows × 141 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-075fe436-7077-46fc-96c3-f51a9be028a9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-075fe436-7077-46fc-96c3-f51a9be028a9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-075fe436-7077-46fc-96c3-f51a9be028a9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YSAHMnP2pu81"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}